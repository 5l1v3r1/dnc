{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat Copy Task\n",
    "### Using a Differentiable Memory Architecture (DNC)\n",
    "\n",
    "<a href=\"http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html\"><img src=\"./static/dnc_schema.png\" alt=\"DNC schema\" style=\"width: 700px;\"/></a>\n",
    "\n",
    "**Sam Greydanus $\\cdot$ February 2017 $\\cdot$ MIT License.**\n",
    "\n",
    "Represents the state of the art in differentiable memory. Inspired by this [Nature paper](http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html). Some ideas taken from [this Gihub repo](https://github.com/Mostafa-Samir/DNC-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../dnc')\n",
    "\n",
    "from dnc import DNC\n",
    "from nn_controller import NNController\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xydim = 6\n",
    "tf.app.flags.DEFINE_integer(\"xlen\", xydim, \"Input dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"ylen\", xydim, \"output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"length\", 5, \"Sequence length\")\n",
    "tf.app.flags.DEFINE_integer(\"reps\", 3, \"Number of repeats for copy task\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 1, \"Size of batch in minibatch gradient descent\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"R\", 1, \"Number of DNC read heads\")\n",
    "tf.app.flags.DEFINE_integer(\"W\", 10, \"Word length for DNC memory\")\n",
    "tf.app.flags.DEFINE_integer(\"N\", 7, \"Number of words the DNC memory can store\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"print_every\", 100, \"Print training info after this number of train steps\")\n",
    "tf.app.flags.DEFINE_integer(\"iterations\", 40000, \"Number of training iterations\")\n",
    "tf.app.flags.DEFINE_float(\"lr\", 1e-4, \"Learning rate (alpha) for the model\")\n",
    "tf.app.flags.DEFINE_float(\"momentum\", .9, \"RMSProp momentum\")\n",
    "tf.app.flags.DEFINE_integer(\"save_every\", 1000, \"Save model after this number of train steps\")\n",
    "tf.app.flags.DEFINE_string(\"save_path\", \"models/model.ckpt\", \"Where to save checkpoints\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAABwCAYAAAAKXJmJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAADxVJREFUeJzt3W2sHNV5wPH/c12wEwhGxSlYJQJSkja+jmhsh5QmvDQ2\nMqIqtFJFcKkqXiIEJP3gqi1BIJFSStJGBFqIq6oNOBXh0lRtFFADBgNtigilsRNaX0NReAm02BSD\ncCRju9j39MOsyX3be/fsndndmfv/Sfvh7p6dfc48Z+fc2Zl5JlJKSJKk+hrqdwCSJGlunMwlSao5\nJ3NJkmrOyVySpJpzMpckqeaczCVJqjknc0mSas7JXJKkmnMylySp5n6qyoVHxDHAWuBFYF+VnyVJ\nUsMsAk4ENqWUXp+pYVeTeUR8Bvh94DjgKeB3U0r/Pk3TtcDXu/kMSZIEwEXA3TM1yJ7MI+JTwM3A\n5cCTwHpgU0R8MKW0a1LzFwHOv+vXWPKhJRNeeGj9Zs6+ZU3ux89dZi36oVUbs9qPbblk2uf71t+K\ntVud7fp7x6rOh9xlWw50G1ZnMsfCgjZj4QHgnOkWnxnO2Pcuzmpf1tjM1S63ubd5uGPVYZmfnPcB\nl36v8/HTflxuotgnmajysdknTd1OtTPo/d319C6+9dv3QWsunUk3e+brgb9KKf0tQERcAfwqcCnw\nZ5Pa7gNY8qElLF1x3IQXFi5eOOW5nsidzDMXP9amT33rb8Xarc5FR7frb+cb8KUr3u4uqE7lTuZt\nnl8ELJ1u8ZnhtBs77ZQ1NnO1G8v592w6PLN93gfkjZ9243L67FY+NvukqdupdmrU31kPU2dtDyLi\nMGAl8PCh51Jx27XNwGm50UmSpLnL/ed+CcUOyquTnn+V4vi5JEnqMS9NkySp5nKPme8CDgLHTnr+\nWGBnuzc9tH4zCxcvnPDc4hOOyvzoehtet6zfIfTU8IXzp7/L+x1Aj82vsTy/sju/cjtY/d02Msro\nyPYJz+3fvb/j92dN5imltyNiC7AauBcgIqL191+0e9/Zt6ypy0kGlVm+brjfIfTU8Dzq74f7HUCP\nza+xPL8m8/mV28Hq7/J1w1Pi2bF1J19deWdH7+/mbPYvAxtbk/qhS9PeDWzsYlmSJGmOsifzlNI3\nImIJcAPFz+s/ANamlF4rOzhJkjS7rirApZQ2ABtKjkWSJHXBs9klSaq5SJllmyLidOAPKIrHLAV+\nPaV0b5u2K4Atl225pL4nwOWWtYqoJo6GyFmdA7cq80uc5cntcM3H5iCtzspXZW7lyaEv5i0/M56x\ng5/Laj+0IC+esbFrstrnys3Xnwxllg7OXJ/XHsyrCNh5PDuAvwZYmVLaOlPLbvbMj6A4Tn4V+RUr\nJUlSybo5Ae4BintLHLosTZIk9ZHHzCVJqjknc0mSas7JXJKkmuvqOvNc09VmH163bKBK6UmS1D/b\nWo/xZr2N+Tt6Mplbm12SpJksZ+q9AN65NG1W2ZN5RBwBnMxPrsR7f0ScAryRUno5d3mSJGluutkz\nXwU8SnGNeQJubj3/NeDSkuKSJEkd6uY683/BE+ckSRoYTsqSJNVc1p55RFwD/AbwC8Be4HHg6pTS\ns6VFlFvDOLdmcG5N4qprJA9aDeMFeTWMq6tJTPa6vO7g/2W1r3zsDNryM8fawI2dCpefu+zrxvJi\nzy3mnjsWFmSOharHZtVya4/mj7XDM9tnjs0Ox8+OrQe4Y1Vny8zdMz8duA34GLAGOAx4MCLelbkc\nSZJUkqw985TSueP/joiLgf+luIPaY+WFJUmSOjXXY+ZHU5zR/kYJsUiSpC50PZm37ph2K/BYSml7\neSFJkqQcc6kAtwFYBny8pFgkSVIXuprMI+J24Fzg9JTSjtnaW5tdkqT2RkdGGb1n4o/c+97c3/H7\nuynnejtwPnBmSumlTt5jbXZJktobXjfM8KQd3B1bd3LHqjs7en/udeYbgHXAecCeiDi29dLulFLn\nt3eRJEmlyT0B7grgKOCfgVfGPS4oNyxJktSp3OvMLf8qSdKAcXKWJKnmco+ZXwFcCZzYemoUuCGl\n9EBpEVVcw7jq5efWPK5a1TWMq1x+br3jbJm1x7PHzoGK7wMwaDLXZ/bYOVDhfQAGzVDeyjk4VnHt\n9NxkDZjI3G29dizvPg+5Ol2dOas9d8/8ZeBqYAVFCddHgHsjYlnmciRJUklyj5n/06SnrouIKylu\nvGIVOEmS+qDrCnARMURxFvtC4F9Li0iSJGXppmjMcuC7wCLgLeCClNIPyw5MkiR1ppuz2Z8BTgFO\nBW4H7omIj5QalSRJ6lj2nnlK6QDwfOvP70fEqRRnuF/e7j3WZpckqb1tI6OMjkw89Wz/7gprs09j\nCFgwUwNrs0uS1N7ydcNTdnB3bN3JV1dWU5v9JuB+4CXgPcBFwBnAjTnLkSRJ5cndM/8Z4GvAUmA3\n8B/A2pTSo2UHJkmSOpN7nfmnqwpEkiR1x9rskiTV3JxOgIuIzwE3AbemlH6vbcOUikcHcmubZ9dO\njy9ktSezJPFYxTWSO1yN78itb55bm/3GyKx/nbE+rx3Li6XyuvtDeWMn+74Bmaoea1XX9b8xs3Z6\n7vKzlp071jK/iJXX3c/eTl1TTRxdyt6uVV13P3N9Xpc7firQ9Z55RHyU4nK0p8oLR5Ik5epqMo+I\nI4G7gE8Db5YakSRJytLtnvlXgPtSSo+UGYwkScrXTW32C4FfBFaVH44kScqVWzTmeOBWYE1Kqf9H\n/CVJUvae+UrgvcDWiHfOdV0AnBERnwUWpjT1vMSH1j/MwqMn1Wa/cBnL1y3rImRJkpql17XZNwMf\nnvTcRuBp4IvTTeQAZ9+y2trskiS10dPa7CmlPcCEfx0iYg/wekrp6ZxlSZKkcpRRAS7zcn9JklSm\nOd8CNaX0yTICkSRJ3Yk2h7nLWXjECmDLZVsu6fyYeW48uTUnK+xvN7LL12aWYazz6syNpXJ1XplU\nP9ZyDdLqrHysDdh2p+qxOWjbtapVNX7GHTNfmVLaOlPbrJ/ZI+L6iBib9Ng++zslSVJVuvmZfRuw\nmp+Uoj9QXjiSJClXN5P5gZTSa6VHIkmSutLN2ewfiIj/iYjnIuKuiHhf6VFJkqSO5U7mTwAXA2uB\nK4CTgO9ExBElxyVJkjqUWzRm07g/t0XEk8CPgAuAtmVqHlq/mYWLJ5ZzXXzCUZxz29qcj6+1bSPb\n51X52tGRUYYnVTNqqvmW220jo1MqVTXVfOorzL/+DtJ2qtflXCdIKe2OiGeBk2dqd/Yta6ZcmvZ3\n5/39XD66dkbvmV8b/NF7tg/Ml6Rq8y63I9vnzQZ/PvUV5mF/B2g7NddyrnOqABcRR1JM5DvmshxJ\nktS93OvMvxQRZ0TECRHxy8A3gbeBkUqikyRJs8r9mf144G7gGOA14DHgl1JKr5cdmCRJ6kzuCXDr\nMpe/CGDX07umvLB/9352bN053YfkfcKAlcxsZ/+b0/d3KDOcsenW2Qz6tTr3telvlbFUrk1n2+V2\n0MZmWWOt7Xc30yB91dstu6y+1qX+aFnb5UHbrrVTxnYKqttWjZs7F80aQ8W12X8L+HplHyBJUvNd\nlFK6e6YGVU/mx1Bck/4isK+yD5IkqXkWAScCm2Y7nF3pZC5Jkqo3p0vTJElS/zmZS5JUc07mkiTV\nnJO5JEk115fJPCI+ExEvRMTeiHgiIj7ajziqFhHXR8TYpMf22d85+CLi9Ii4t3U73LGIOG+aNjdE\nxCsR8VZEPBQRM9bwH2Sz9Tci7pwm19/uV7xzERHXRMSTEfHjiHg1Ir4ZER+cpl0j8ttJf5uS34i4\nIiKeiojdrcfjEXHOpDaNyCvM3t+m5BX6MJlHxKeAm4HrgY8ATwGbImJJr2PpkW3AscBxrccn+htO\naY4AfgBcBUy5JCIirgY+C1wOnArsocjz4b0MskQz9rflfibmOrfI0qA4HbgN+BiwBjgMeDAi3nWo\nQcPyO2t/W5qQ35eBq4EVwErgEeDeiFgGjcsrzNLflibkFVJKPX1Q3BP9z8f9HcB/A3/Y61h60Nfr\nga39jqMH/RwDzpv03CvA+nF/HwXsBS7od7wV9fdO4B/7HVtF/V3S6vMn5kl+p+tvk/P7OnBJ0/Pa\npr+NyWtP98wj4jCK/44ePvRcKtboZuC0XsbSQx9o/TT7XETcFRHv63dAVYuIkyj+wx2f5x8D/0Zz\n8wxwVutn2mciYkNE/HS/AyrJ0RS/RrwB8yK/E/o7TqPyGxFDEXEhsBD4TtPzOrm/415qRF7ndD/z\nLiwBFgCvTnr+VeDnexxLLzwBXAz8F7AU+DzFl2Z5SmlPH+Oq2nEUG8Pp8nzc1OaNcD/wD8ALwM8B\nXwC+HRGntf5hraWICOBW4LGU0qHzPRqb3zb9hQblNyKWA9+lqC72FsVe93MRcRoNzGu7/rZebkxe\nez2ZzysppU3j/twWEU8CPwIuoPh5Rw2RUvrGuD9HI+I/geeAs4BH+xJUOTYAy4CP9zuQHpm2vw3L\n7zPAKcBi4DeBeyLizP6GVKlp+5tS+n6T8trrE+B2AQcpTjYY71ighFsTDbaU0m7gWaC2Z4d2aCfF\nuRDzMs8AKaUXKMZ7bXMdEbcD5wJnpZR2jHupkfmdob9T1Dm/KaUDKaXnW5PZtRQ/o19JQ/M6Q3+n\na1vbvPZ0Mk8pvQ1sAVYfeq71s9Zq4PFextIPEXEkxSCZcUNRd60vxE4m5vkoirOFG59ngIg4HjiG\nmua6NbGdD/xKSuml8a81Mb8z9bdN+1rnd5IhYEET89rGEMXh3inqnNd+/Mz+ZWBjRGwBngTWA+8G\nNvYhlkpFxJeA+yh+Wv9Z4I+At4GRfsZVhog4guIfk0N38n1/RJwCvJFSepniuON1EfFDirvm/THF\nVQvf6kO4czZTf1uP6ymOve1stftTil9hNk1d2mCLiA0Ul+ecB+yJiEN7artTSofuftiY/M7W31bu\nG5HfiLiJ4jjxS8B7gIuAM4AbW00ak1eYub9NyivQ+0vTWucUXEUxUPZSnJiwqt+n9VfUzxGKL8Je\nisF0N3BSv+MqqW9nUly+c3DS445xbT5PcanLWxRfjpP7HXcV/aU4seYBig3CPuB54C+B9/Y77i77\nOl0/DwK/M6ldI/I7W3+blF/gb1rx723150Hgk03M62z9bVJeU0reAlWSpLqzNrskSTXnZC5JUs05\nmUuSVHNO5pIk1ZyTuSRJNedkLklSzTmZS5JUc07mkiTVnJO5JEk152QuSVLNOZlLklRzTuaSJNXc\n/wNYM0Jj7W43GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee65190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_sequence(length, reps, dim):\n",
    "    X = [np.concatenate((np.random.randint(2, size=(length,dim)), np.zeros((length + 3,dim)))) for _ in range(reps)]\n",
    "    X = np.vstack(X) ; X[:,dim-1] = 0\n",
    "    \n",
    "    X = np.concatenate((X[-1:,:],X[:-1,:]))\n",
    "    y = np.concatenate((X[-(length + 2):,:],X[:-(length + 2),:]))\n",
    "    markers = range(length+1, X.shape[0], 2*length+3)\n",
    "    X[markers,dim-1] = 1\n",
    "    return X, y\n",
    "        \n",
    "def next_batch(batch_size, length, reps, dim):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        X, y = get_sequence(length, reps, dim)\n",
    "        X_batch.append(X) ; y_batch.append(y)\n",
    "    return [X_batch, y_batch]\n",
    "\n",
    "batch = next_batch(1, FLAGS.length, FLAGS.reps, FLAGS.xlen)\n",
    "plt.imshow(batch[0][0].T - batch[1][0].T, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_hat, y):\n",
    "    return tf.reduce_mean(-y*tf.log(y_hat) - (1-y)*tf.log(1-y_hat))\n",
    "\n",
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph, initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph...\n",
      "defining loss...\n",
      "computing gradients...\n",
      "init variables... \n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "llprint(\"building graph...\\n\")\n",
    "optimizer = tf.train.RMSPropOptimizer(FLAGS.lr, momentum=FLAGS.momentum)\n",
    "dnc = DNC(NNController, FLAGS)\n",
    "\n",
    "llprint(\"defining loss...\\n\")\n",
    "y_hat, outputs = dnc.get_outputs()\n",
    "y_hat = tf.clip_by_value(tf.sigmoid(y_hat), 1e-6, 1. - 1e-6)\n",
    "loss = binary_cross_entropy(y_hat, dnc.y)\n",
    "\n",
    "llprint(\"computing gradients...\\n\")\n",
    "gradients = optimizer.compute_gradients(loss)\n",
    "grad_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "llprint(\"init variables... \\n\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "llprint(\"ready to train...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf parameter overview\n",
    "total_parameters = 0 ; print \"model overview...\"\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print '\\tvariable \"{}\" has {} parameters' \\\n",
    "        .format(variable.name, variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print \"total of {} parameters\".format(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(FLAGS.save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print \"no saved model to load.\"\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print \"loaded model: {}\".format(load_path)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    global_step = int(load_path.split('-')[-1]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for i in xrange(global_step, FLAGS.iterations + 1):\n",
    "    llprint(\"\\rIteration {}/{}\".format(i, FLAGS.iterations))\n",
    "\n",
    "    rlen = np.random.randint(1, FLAGS.length + 1)\n",
    "    rreps = np.random.randint(1, FLAGS.reps + 1)\n",
    "    X, y = next_batch(FLAGS.batch_size, rlen, rreps, FLAGS.xlen)\n",
    "    tsteps = rreps*(2*rlen+3)\n",
    "\n",
    "    fetch = [loss, grad_op]\n",
    "    feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "\n",
    "    step_loss, _ = sess.run(fetch, feed_dict=feed)\n",
    "    loss_history.append(step_loss)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        llprint(\"\\n\\tloss: {:03.4f}\\n\".format(np.mean(loss_history)))\n",
    "        loss_history = []\n",
    "    if i % FLAGS.save_every == 0 and i is not 0:\n",
    "        llprint(\"\\n\\tSAVING MODEL\\n\")\n",
    "        saver.save(sess, FLAGS.save_path, global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.reps, FLAGS.xlen)\n",
    "tsteps = FLAGS.reps*(2*FLAGS.length+3)\n",
    "\n",
    "feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "fetch = [outputs['y_hat'], outputs['w_w'], outputs['w_r'], outputs['f'], outputs['g_a']]\n",
    "[_y_hat, _w_w, _w_r, _f, _g_a] = sess.run(fetch, feed)\n",
    "_y = y[0] ; _X = X[0]\n",
    "\n",
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "plt.subplot(611) ; plt.title('y')\n",
    "plt.imshow(_y.T, interpolation='none')\n",
    "plt.subplot(612) ; plt.title('y_hat')\n",
    "plt.imshow(_y_hat[0,:,:].T, interpolation='none')\n",
    "plt.subplot(613) ; plt.title('w_W')\n",
    "plt.imshow(_w_w[0,:,:].T, interpolation='none')\n",
    "plt.subplot(614) ; plt.title('w_r')\n",
    "plt.imshow(_w_r[0,:,:,0].T, interpolation='none')\n",
    "plt.subplot(615) ; plt.title('free gate')\n",
    "plt.imshow(_f[0,:,:].T, interpolation='none')\n",
    "plt.subplot(616) ; plt.title('alloc. gate')\n",
    "plt.imshow(_g_a[0,:,:].T, interpolation='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
