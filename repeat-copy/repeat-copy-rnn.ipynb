{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat Copy Task\n",
    "### Differentiable Neural Computer (DNC) using a RNN Controller\n",
    "\n",
    "<a href=\"http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html\"><img src=\"../static/dnc_schema.png\" alt=\"DNC schema\" style=\"width: 700px;\"/></a>\n",
    "\n",
    "**Sam Greydanus $\\cdot$ February 2017 $\\cdot$ MIT License.**\n",
    "\n",
    "Represents the state of the art in differentiable memory. Inspired by this [Nature paper](http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html). Some ideas taken from [this Gihub repo](https://github.com/Mostafa-Samir/DNC-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../dnc')\n",
    "\n",
    "from dnc import DNC\n",
    "from rnn_controller import RNNController\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xydim = 6\n",
    "tf.app.flags.DEFINE_integer(\"xlen\", xydim, \"Input dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"ylen\", xydim, \"output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"length\", 5, \"Sequence length\")\n",
    "tf.app.flags.DEFINE_integer(\"reps\", 3, \"Number of repeats for copy task\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 1, \"Size of batch in minibatch gradient descent\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"R\", 1, \"Number of DNC read heads\")\n",
    "tf.app.flags.DEFINE_integer(\"W\", 10, \"Word length for DNC memory\")\n",
    "tf.app.flags.DEFINE_integer(\"N\", 7, \"Number of words the DNC memory can store\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"print_every\", 100, \"Print training info after this number of train steps\")\n",
    "tf.app.flags.DEFINE_integer(\"iterations\", 10000, \"Number of training iterations\")\n",
    "tf.app.flags.DEFINE_float(\"lr\", 1e-4, \"Learning rate (alpha) for the model\")\n",
    "tf.app.flags.DEFINE_float(\"momentum\", .9, \"RMSProp momentum\")\n",
    "tf.app.flags.DEFINE_integer(\"save_every\", 1000, \"Save model after this number of train steps\")\n",
    "tf.app.flags.DEFINE_string(\"save_path\", \"nn_models/model.ckpt\", \"Where to save checkpoints\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAABwCAYAAAAKXJmJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqeYczCVJqjkHc0mSau7/APAgGuehUE1gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10964f650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_sequence(length, reps, dim):\n",
    "    X = [np.concatenate((np.random.randint(2, size=(length,dim)), np.zeros((length + 3,dim)))) for _ in range(reps)]\n",
    "    X = np.vstack(X) ; X[:,dim-1] = 0\n",
    "    \n",
    "    X = np.concatenate((X[-1:,:],X[:-1,:]))\n",
    "    y = np.concatenate((X[-(length + 2):,:],X[:-(length + 2),:]))\n",
    "    markers = range(length+1, X.shape[0], 2*length+3)\n",
    "    X[markers,dim-1] = 1\n",
    "    return X, y\n",
    "        \n",
    "def next_batch(batch_size, length, reps, dim):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        X, y = get_sequence(length, reps, dim)\n",
    "        X_batch.append(X) ; y_batch.append(y)\n",
    "    return [X_batch, y_batch]\n",
    "\n",
    "batch = next_batch(1, FLAGS.length, FLAGS.reps, FLAGS.xlen)\n",
    "plt.imshow(batch[0][0].T - batch[1][0].T, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_hat, y):\n",
    "    return tf.reduce_mean(-y*tf.log(y_hat) - (1-y)*tf.log(1-y_hat))\n",
    "\n",
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph, initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph...\n",
      "LSTMStateTuple(c=<tf.Tensor 'while_loop/while/Identity_1:0' shape=(1, 64) dtype=float32>, h=<tf.Tensor 'while_loop/while/Identity_2:0' shape=(1, 64) dtype=float32>) LSTMStateTuple(c=<tf.Tensor 'while_loop/while/basic_lstm_cell/add_1:0' shape=(1, 64) dtype=float32>, h=<tf.Tensor 'while_loop/while/basic_lstm_cell/mul_2:0' shape=(1, 64) dtype=float32>)\n",
      "defining loss...\n",
      "computing gradients...\n",
      "init variables... \n",
      "ready to train..."
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "llprint(\"building graph...\\n\")\n",
    "optimizer = tf.train.RMSPropOptimizer(FLAGS.lr, momentum=FLAGS.momentum)\n",
    "dnc = DNC(RNNController, FLAGS)\n",
    "\n",
    "llprint(\"defining loss...\\n\")\n",
    "y_hat, outputs = dnc.get_outputs()\n",
    "y_hat = tf.clip_by_value(tf.sigmoid(y_hat), 1e-6, 1. - 1e-6)\n",
    "loss = binary_cross_entropy(y_hat, dnc.y)\n",
    "\n",
    "llprint(\"computing gradients...\\n\")\n",
    "gradients = optimizer.compute_gradients(loss)\n",
    "for i, (grad, var) in enumerate(gradients):\n",
    "    if grad is not None:\n",
    "        gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n",
    "                    \n",
    "grad_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "llprint(\"init variables... \\n\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "llprint(\"ready to train...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model overview...\n",
      "\tvariable \"basic_lstm_cell/weights:0\" has 20480 parameters\n",
      "\tvariable \"basic_lstm_cell/biases:0\" has 256 parameters\n",
      "\tvariable \"W_z:0\" has 3072 parameters\n",
      "\tvariable \"W_v:0\" has 384 parameters\n",
      "\tvariable \"W_r:0\" has 60 parameters\n",
      "\tvariable \"while_loop/basic_lstm_cell/weights:0\" has 20480 parameters\n",
      "\tvariable \"while_loop/basic_lstm_cell/biases:0\" has 256 parameters\n",
      "total of 44988 parameters\n"
     ]
    }
   ],
   "source": [
    "# tf parameter overview\n",
    "total_parameters = 0 ; print \"model overview...\"\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print '\\tvariable \"{}\" has {} parameters' \\\n",
    "        .format(variable.name, variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print \"total of {} parameters\".format(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model to load.\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(FLAGS.save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print \"no saved model to load.\"\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print \"loaded model: {}\".format(load_path)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    global_step = int(load_path.split('-')[-1]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/40000\n",
      "\tloss: 0.6937\n",
      "Iteration 100/40000\n",
      "\tloss: 0.6921\n",
      "Iteration 200/40000\n",
      "\tloss: 0.4698\n",
      "Iteration 300/40000\n",
      "\tloss: 0.3195\n",
      "Iteration 400/40000\n",
      "\tloss: 0.2620\n",
      "Iteration 500/40000\n",
      "\tloss: 0.2429\n",
      "Iteration 600/40000\n",
      "\tloss: 0.2198\n",
      "Iteration 700/40000\n",
      "\tloss: 0.2103\n",
      "Iteration 800/40000\n",
      "\tloss: 0.1918\n",
      "Iteration 831/40000"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for i in xrange(global_step, FLAGS.iterations + 1):\n",
    "    llprint(\"\\rIteration {}/{}\".format(i, FLAGS.iterations))\n",
    "\n",
    "    rlen = np.random.randint(1, FLAGS.length + 1)\n",
    "    rreps = np.random.randint(1, FLAGS.reps + 1)\n",
    "    X, y = next_batch(FLAGS.batch_size, rlen, rreps, FLAGS.xlen)\n",
    "    tsteps = rreps*(2*rlen+3)\n",
    "\n",
    "    fetch = [loss, grad_op]\n",
    "    feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "\n",
    "    step_loss, _ = sess.run(fetch, feed_dict=feed)\n",
    "    loss_history.append(step_loss)\n",
    "    global_step = i\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        llprint(\"\\n\\tloss: {:03.4f}\\n\".format(np.mean(loss_history)))\n",
    "        loss_history = []\n",
    "    if i % FLAGS.save_every == 0 and i is not 0:\n",
    "        llprint(\"\\n\\tSAVING MODEL\\n\")\n",
    "        saver.save(sess, FLAGS.save_path, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.reps, FLAGS.xlen)\n",
    "tsteps = FLAGS.reps*(2*FLAGS.length+3)\n",
    "\n",
    "feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "fetch = [outputs['y_hat'], outputs['w_w'], outputs['w_r'], outputs['f'], outputs['g_a']]\n",
    "[_y_hat, _w_w, _w_r, _f, _g_a] = sess.run(fetch, feed)\n",
    "_y = y[0] ; _X = X[0]\n",
    "\n",
    "fig, ((ax1,ax2),(ax3,ax5),(ax4,ax6),) = plt.subplots(nrows=3, ncols=2)\n",
    "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
    "fs = 12 # font size\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "ax1.imshow(_X.T - _y.T, interpolation='none') ; ax1.set_title('input ($X$) and target ($y$)')\n",
    "ax2.imshow(_y_hat[0,:,:].T, interpolation='none') ; ax2.set_title('prediction ($\\hat y$)')\n",
    "\n",
    "ax3.imshow(_w_w[0,:,:].T, interpolation='none') ; ax3.set_title('write weighting ($w_w$)')\n",
    "ax4.imshow(_w_r[0,:,:,0].T, interpolation='none') ; ax4.set_title('read weighting ($w_r$)')\n",
    "\n",
    "ax5.imshow(_f[0,:,:].T, interpolation='none') ; ax5.set_title('free gate ($f$)') ; ax5.set_aspect(3)\n",
    "ax6.imshow(_g_a[0,:,:].T, interpolation='none') ; ax6.set_title('allocation gate ($g_a$)') ; ax6.set_aspect(3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
