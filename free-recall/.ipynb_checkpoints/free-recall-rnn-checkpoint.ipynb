{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat Copy Task\n",
    "### Differentiable Neural Computer (DNC) using a RNN Controller\n",
    "\n",
    "<a href=\"https://goo.gl/6eiJFc\"><img src=\"../static/dnc_schema.png\" alt=\"DNC schema\" style=\"width: 700px;\"/></a>\n",
    "\n",
    "**Sam Greydanus $\\cdot$ February 2017 $\\cdot$ MIT License.**\n",
    "\n",
    "Represents the state of the art in differentiable memory. Inspired by this [Nature paper](https://goo.gl/6eiJFc). Some ideas taken from [this Gihub repo](https://github.com/Mostafa-Samir/DNC-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, there are interesting parallels between the memory mechanisms of a DNC and the functional capabilities of the mammalian hippocampus. DNC memory modification is fast and can be one-shot, resembling the associative long-term potentiation of hippocampal CA3 and CA1 synapses. The hippocampal dentate gyrus, a region known to support neurogenesis, has been proposed to increase representational sparsity, thereby enhancing memory capacity: usage- based memory allocation and sparse weightings may provide similar facilities in our model. Human 'free recall' experiments demonstrate the increased probability of item recall in the same order as first presentedâ€”a hippocampus-dependent phenomenon accounted for by the temporal context model, bearing some similarity to the formation of temporal links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../dnc')\n",
    "\n",
    "from dnc import DNC\n",
    "from rnn_controller import RNNController\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xydim = 6\n",
    "tf.app.flags.DEFINE_integer(\"xlen\", xydim, \"Input dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"ylen\", xydim, \"output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"length\", 5, \"Sequence length\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 3, \"Size of batch in minibatch gradient descent\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"R\", 1, \"Number of DNC read heads\")\n",
    "tf.app.flags.DEFINE_integer(\"W\", 10, \"Word length for DNC memory\")\n",
    "tf.app.flags.DEFINE_integer(\"N\", 7, \"Number of words the DNC memory can store\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"print_every\", 100, \"Print training info after this number of train steps\")\n",
    "tf.app.flags.DEFINE_integer(\"iterations\", 30000, \"Number of training iterations\")\n",
    "tf.app.flags.DEFINE_float(\"lr\", 1e-4, \"Learning rate (alpha) for the model\")\n",
    "tf.app.flags.DEFINE_float(\"momentum\", .9, \"RMSProp momentum\")\n",
    "tf.app.flags.DEFINE_integer(\"save_every\", 1000, \"Save model after this number of train steps\")\n",
    "tf.app.flags.DEFINE_string(\"save_path\", \"rnn_models/model.ckpt\", \"Where to save checkpoints\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAD+CAYAAAAu5uwhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnpY5DwLckwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085f86d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_sequence(length, dim):\n",
    "    X = np.concatenate((np.random.randint(2, size=(length,dim)), np.zeros((length + 3,dim))))\n",
    "    X = np.vstack(X) ; X[:,dim-1] = 0\n",
    "    \n",
    "    X = np.concatenate((X[-1:,:],X[:-1,:]))\n",
    "    y = np.concatenate((X[-(length + 2):,:],X[:-(length + 2),:]))\n",
    "    markers = range(length+1, X.shape[0], 2*length+3)\n",
    "    X[markers,dim-1] = 1\n",
    "    return X, y\n",
    "        \n",
    "def next_batch(batch_size, length, dim):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        X, y = get_sequence(length, dim)\n",
    "        X_batch.append(X) ; y_batch.append(y)\n",
    "    return [X_batch, y_batch]\n",
    "\n",
    "batch = next_batch(1, FLAGS.length, FLAGS.xlen)\n",
    "plt.imshow(batch[0][0].T - batch[1][0].T, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y, y_hat):\n",
    "    return tf.reduce_mean(-y*tf.log(y_hat) - (1-y)*tf.log(1-y_hat))\n",
    "\n",
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_recall_loss(y, y_hat, tsteps):    \n",
    "    # sorry this dimension stuff is uuuuugly but we have to because it's batched\n",
    "    y = tf.expand_dims(y, [1])\n",
    "    y_hat = tf.expand_dims(y_hat, [1])\n",
    "    \n",
    "    y_hat = tf.tile(y_hat,[1,tsteps,1,1])\n",
    "    y_hat = tf.transpose(y_hat, [0,2,1,3])\n",
    "    \n",
    "    y_minus = -y*tf.log(y_hat) - (1-y)*tf.log(1-y_hat) # binary cross entropy loss\n",
    "    y_minus = tf.reduce_sum(y_minus, axis=-1)\n",
    "    y_minus = tf.reduce_min(y_minus, axis=1)\n",
    "    \n",
    "    return tf.reduce_sum(y_minus) / (FLAGS.batch_size*FLAGS.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.07968e-06\n"
     ]
    }
   ],
   "source": [
    "X, real_y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.xlen)\n",
    "real_y = np.stack(real_y)[:,-FLAGS.length:,:]\n",
    "\n",
    "pred_y = [y_i[np.random.permutation(y_i.shape[0]),:] for y_i in real_y]\n",
    "pred_y = np.stack(pred_y)\n",
    "# pred_y[:,-2:,-1] = 4\n",
    "\n",
    "tf_real_y = tf.placeholder(tf.float32, [FLAGS.batch_size, None, FLAGS.ylen], name='real_y')\n",
    "tf_pred_y = tf.placeholder(tf.float32, [FLAGS.batch_size, None, FLAGS.ylen], name='pred_y')\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "# clip predictions to range (0,1) to avoid infinite losses\n",
    "_tf_pred_y = tf.clip_by_value(tf_pred_y, 1e-6, 1-1e-6)\n",
    "    \n",
    "l = free_recall_loss(tf_real_y, _tf_pred_y, FLAGS.length)\n",
    "feed = {tf_real_y: real_y, tf_pred_y: pred_y}\n",
    "print l.eval(feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph, initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph...\n",
      "defining loss...\n",
      "computing gradients...\n",
      "init variables... \n",
      "ready to train..."
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "llprint(\"building graph...\\n\")\n",
    "optimizer = tf.train.RMSPropOptimizer(FLAGS.lr, momentum=FLAGS.momentum)\n",
    "dnc = DNC(RNNController, FLAGS)\n",
    "\n",
    "llprint(\"defining loss...\\n\")\n",
    "y_hat, outputs = dnc.get_outputs()\n",
    "y_hat = tf.clip_by_value(tf.sigmoid(y_hat), 1e-6, 1. - 1e-6) # avoid infinity\n",
    "rlen = (dnc.tsteps-3)/2\n",
    "loss = free_recall_loss(dnc.y[:,-rlen:,:], y_hat[:,-rlen:,:], tsteps=rlen)\n",
    "\n",
    "llprint(\"computing gradients...\\n\")\n",
    "gradients = optimizer.compute_gradients(loss)\n",
    "for i, (grad, var) in enumerate(gradients):\n",
    "    if grad is not None:\n",
    "        gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n",
    "                    \n",
    "grad_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "llprint(\"init variables... \\n\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "llprint(\"ready to train...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model overview...\n",
      "\tvariable \"dnc_scope/basic_lstm_cell/weights:0\" has 73728 parameters\n",
      "\tvariable \"dnc_scope/basic_lstm_cell/biases:0\" has 512 parameters\n",
      "\tvariable \"W_z:0\" has 6144 parameters\n",
      "\tvariable \"W_v:0\" has 768 parameters\n",
      "\tvariable \"W_r:0\" has 60 parameters\n",
      "total of 81212 parameters\n"
     ]
    }
   ],
   "source": [
    "# tf parameter overview\n",
    "total_parameters = 0 ; print \"model overview...\"\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print '\\tvariable \"{}\" has {} parameters' \\\n",
    "        .format(variable.name, variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print \"total of {} parameters\".format(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: rnn_models/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(FLAGS.save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print \"no saved model to load.\"\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print \"loaded model: {}\".format(load_path)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    global_step = int(load_path.split('-')[-1]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2100/10000\n",
      "\tloss: 1.1709\n",
      "Iteration 2200/10000\n",
      "\tloss: 1.1148\n",
      "Iteration 2300/10000\n",
      "\tloss: 1.0109\n",
      "Iteration 2400/10000\n",
      "\tloss: 1.0461\n",
      "Iteration 2500/10000\n",
      "\tloss: 1.1705\n",
      "Iteration 2600/10000\n",
      "\tloss: 0.9667\n",
      "Iteration 2685/10000"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for i in xrange(global_step, FLAGS.iterations + 1):\n",
    "    llprint(\"\\rIteration {}/{}\".format(i, FLAGS.iterations))\n",
    "\n",
    "    rlen = np.random.randint(1, FLAGS.length + 1)\n",
    "    X, y = next_batch(FLAGS.batch_size, rlen, FLAGS.xlen)\n",
    "    tsteps = 2*rlen+3\n",
    "\n",
    "    fetch = [loss, grad_op]\n",
    "    feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "\n",
    "    step_loss, _ = sess.run(fetch, feed_dict=feed)\n",
    "    loss_history.append(step_loss)\n",
    "    global_step = i\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        llprint(\"\\n\\tloss: {:03.4f}\\n\".format(np.mean(loss_history)))\n",
    "        loss_history = []\n",
    "    if i % FLAGS.save_every == 0 and i is not 0:\n",
    "        llprint(\"\\n\\tSAVING MODEL\\n\")\n",
    "        saver.save(sess, FLAGS.save_path, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.xlen)\n",
    "tsteps = 2*FLAGS.length+3\n",
    "\n",
    "feed = {dnc.X: X, dnc.y: y, dnc.tsteps: tsteps}\n",
    "fetch = [outputs['y_hat'], outputs['w_w'], outputs['w_r'], outputs['f'], outputs['g_a']]\n",
    "[_y_hat, _w_w, _w_r, _f, _g_a] = sess.run(fetch, feed)\n",
    "_y_hat = np.clip(_y_hat, 1e-6, 1-1e-6)\n",
    "_y = y[0] ; _X = X[0]\n",
    "\n",
    "fig, ((ax1,ax2),(ax3,ax5),(ax4,ax6),) = plt.subplots(nrows=3, ncols=2)\n",
    "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
    "fs = 12 # font size\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "ax1.imshow(_X.T - _y.T, interpolation='none') ; ax1.set_title('input ($X$) and target ($y$)')\n",
    "ax2.imshow(_y_hat[0,-FLAGS.length:,:].T, interpolation='none') ; ax2.set_title('prediction ($\\hat y$)')\n",
    "\n",
    "ax3.imshow(_w_w[0,:,:].T, interpolation='none') ; ax3.set_title('write weighting ($w_w$)')\n",
    "ax4.imshow(_w_r[0,:,:,0].T, interpolation='none') ; ax4.set_title('read weighting ($w_r$)')\n",
    "\n",
    "ax5.imshow(_f[0,:,:].T, interpolation='none') ; ax5.set_title('free gate ($f$)') ; ax5.set_aspect(3)\n",
    "ax6.imshow(_g_a[0,:,:].T, interpolation='none') ; ax6.set_title('allocation gate ($g_a$)') ; ax6.set_aspect(3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X, y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.xlen)\n",
    "# y = np.stack(y)\n",
    "# y = y[:,FLAGS.length + 3:,:]\n",
    "# y_shuffle = y[:,np.random.permutation(FLAGS.length),:]\n",
    "# y_shuffle[:,-1,-1] = 4\n",
    "\n",
    "# plt.figure(0, figsize=[2,2])\n",
    "# plt.imshow(y[0,:,:].T, interpolation='none')\n",
    "# plt.figure(1, figsize=[2,2])\n",
    "# plt.imshow(y_shuffle[0,:,:].T, interpolation='none')\n",
    "# plt.show()\n",
    "\n",
    "# def np_loss(real_y, pred_y, FLAGS):\n",
    "#     # sorry this is uuuuugly but we have to because it's batched\n",
    "#     real_y = np.tile(real_y, [1,1,1,1]) ; real_y = np.transpose(real_y, (1,0,2,3))\n",
    "#     pred_y = np.tile(pred_y,[1,1,1,1]) ; pred_y = np.transpose(pred_y, (1,0,2,3))\n",
    "    \n",
    "#     pred_y = np.tile(pred_y,[1,FLAGS.xlen-1,1,1])\n",
    "#     pred_y = np.transpose(pred_y, (0,2,1,3))\n",
    "    \n",
    "#     real_y = real_y[0,:,:,:] ; pred_y = pred_y[0,:,:,:]\n",
    "#     y_minus = .5*(real_y - pred_y)**2\n",
    "#     y_minus = np.sum(y_minus, axis=-1)\n",
    "#     y_minus = np.min(y_minus, axis=1)\n",
    "    \n",
    "#     return np.sum(y_minus) / (FLAGS.batch_size*FLAGS.length)\n",
    "\n",
    "# X, real_y = next_batch(FLAGS.batch_size, FLAGS.length, FLAGS.xlen)\n",
    "# real_y = np.stack(real_y)[:,-FLAGS.length:,:]\n",
    "\n",
    "# pred_y = [y_i[np.random.permutation(y_i.shape[0]),:] for y_i in real_y]\n",
    "# pred_y = np.stack(pred_y)\n",
    "# # pred_y[:,-2:,-1] = 4\n",
    "\n",
    "# np_loss(real_y, pred_y, FLAGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
